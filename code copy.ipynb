{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import msal\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load bi·∫øn m√¥i tr∆∞·ªùng\n",
    "load_dotenv()\n",
    "\n",
    "# Azure Client ID, Tenant ID, Client Secret t·ª´ file .env\n",
    "azure_client_id = os.getenv(\"AZURE_CLIENT_ID\")\n",
    "azure_tenant_id = os.getenv(\"AZURE_TENANT_ID\")\n",
    "azure_client_secret = os.getenv(\"AZURE_CLIENT_SECRET\")\n",
    "\n",
    "# SharePoint Site URL v√† ID c·ªßa Drive\n",
    "sharepoint_site_url = \"maithujsc.sharepoint.com/sites/Trainingdocument\"\n",
    "drive_id = \"b!SJpkxkt_aECkl7ZK6YMWBTM-60BFIl5ChlC_cxyDngG7XD9-vWJITZvMeqzfYkAW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_access_token():\n",
    "    \"\"\"L·∫•y token truy c·∫≠p Microsoft Graph API\"\"\"\n",
    "    app = msal.ConfidentialClientApplication(\n",
    "        azure_client_id,\n",
    "        authority=f\"https://login.microsoftonline.com/{azure_tenant_id}\",\n",
    "        client_credential=azure_client_secret\n",
    "    )\n",
    "    token = app.acquire_token_for_client(scopes=[\"https://graph.microsoft.com/.default\"])\n",
    "    return token[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_in_folder():\n",
    "    \"\"\"L·∫•y danh s√°ch c√°c file PDF trong th∆∞ m·ª•c\"\"\"\n",
    "    token = get_access_token()\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    \n",
    "    # URL ƒë·ªÉ l·∫•y c√°c file trong th∆∞ m·ª•c g·ªëc c·ªßa drive\n",
    "    url = f\"https://graph.microsoft.com/v1.0/drives/{drive_id}/root/children\"\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # In th√™m th√¥ng tin debug\n",
    "    print(f\"Response status code: {response.status_code}\")\n",
    "    if response.status_code == 200:\n",
    "        files = response.json().get(\"value\", [])\n",
    "        pdf_files = [file[\"name\"] for file in files if file[\"name\"].endswith(\".pdf\")]\n",
    "        print(f\"üìÇ T√¨m th·∫•y {len(pdf_files)} file PDF:\", pdf_files)\n",
    "        return pdf_files\n",
    "    else:\n",
    "        print(\"‚ùå L·ªói l·∫•y danh s√°ch file:\", response.json())\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status code: 200\n",
      "üìÇ T√¨m th·∫•y 5 file PDF: ['01. Mai Thu Packaging.pdf', 'Dao tao van hoa hoi nhap Mai Thu.pdf', 'H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng b·ªô app qu·∫£n l√Ω.pdf', 'H∆Ø·ªöNG D·∫™N T·∫†O CH·ªÆ K√ù EMAIL M·ªöI (1).pdf', 'Qu·∫£n l√Ω ƒë∆°n h√†ng-phi·∫øu SX - Power Apps.pdf']\n",
      "‚úÖ ƒê√£ t·∫£i file: 01. Mai Thu Packaging.pdf\n",
      "‚úÖ ƒê√£ t·∫£i file: Dao tao van hoa hoi nhap Mai Thu.pdf\n",
      "‚úÖ ƒê√£ t·∫£i file: H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng b·ªô app qu·∫£n l√Ω.pdf\n",
      "‚úÖ ƒê√£ t·∫£i file: H∆Ø·ªöNG D·∫™N T·∫†O CH·ªÆ K√ù EMAIL M·ªöI (1).pdf\n",
      "‚úÖ ƒê√£ t·∫£i file: Qu·∫£n l√Ω ƒë∆°n h√†ng-phi·∫øu SX - Power Apps.pdf\n"
     ]
    }
   ],
   "source": [
    "def download_file(file_name):\n",
    "    \"\"\"T·∫£i file PDF t·ª´ SharePoint\"\"\"\n",
    "    token = get_access_token()\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    url = f\"https://graph.microsoft.com/v1.0/drives/{drive_id}/root:/{file_name}:/content\"\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        # L∆∞u file v√†o th∆∞ m·ª•c downloads\n",
    "        with open(f\"downloads/{file_name}\", \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"‚úÖ ƒê√£ t·∫£i file: {file_name}\")\n",
    "    else:\n",
    "        print(f\"‚ùå L·ªói t·∫£i file {file_name}: {response.json()}\")\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c \"downloads\" n·∫øu ch∆∞a c√≥\n",
    "os.makedirs(\"downloads\", exist_ok=True)\n",
    "\n",
    "# L·∫•y danh s√°ch file PDF v√† t·∫£i xu·ªëng\n",
    "pdf_files = get_files_in_folder()\n",
    "for file in pdf_files:\n",
    "    download_file(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import openai\n",
    "import json\n",
    "import os\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    \"\"\"Tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ file PDF\"\"\"\n",
    "    doc = fitz.open(pdf_file)  # M·ªü file PDF\n",
    "    text = \"\"\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()  # Tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ m·ªói trang\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "# T·∫°o embedding t·ª´ vƒÉn b·∫£n\n",
    "def get_embedding(text):\n",
    "    response = openai.Embedding.create(model=\"text-embedding-ada-002\", input=text)\n",
    "    embedding = response[\"data\"][0][\"embedding\"]\n",
    "\n",
    "    return response.data[0].embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdf_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m chroma_client \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mPersistentClient(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./chroma_db\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# T·∫°o th∆∞ m·ª•c l∆∞u tr·ªØ c∆° s·ªü d·ªØ li·ªáu\u001b[39;00m\n\u001b[0;32m      4\u001b[0m collection \u001b[38;5;241m=\u001b[39m chroma_client\u001b[38;5;241m.\u001b[39mget_or_create_collection(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_docs\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# T·∫°o collection l∆∞u t√†i li·ªáu ƒë√†o t·∫°o\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpdf_files\u001b[49m:\n\u001b[0;32m      7\u001b[0m     pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownloads/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m     text \u001b[38;5;241m=\u001b[39m extract_text_from_pdf(pdf_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pdf_files' is not defined"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")  # T·∫°o th∆∞ m·ª•c l∆∞u tr·ªØ c∆° s·ªü d·ªØ li·ªáu\n",
    "collection = chroma_client.get_or_create_collection(name=\"training_docs\")  # T·∫°o collection l∆∞u t√†i li·ªáu ƒë√†o t·∫°o\n",
    "\n",
    "for file_name in pdf_files:\n",
    "    pdf_path = f\"downloads/{file_name}\"\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    if text.strip():  # Ki·ªÉm tra xem file c√≥ n·ªôi dung kh√¥ng\n",
    "        collection.add(\n",
    "            documents=[text],\n",
    "            embeddings=[get_embedding(text)],\n",
    "            ids=[file_name]\n",
    "        )\n",
    "        print(f\"‚úÖ ƒê√£ l∆∞u v√†o ChromaDB: {file_name}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è File {file_name} kh√¥ng c√≥ n·ªôi dung!\")\n",
    "        \n",
    "# Ki·ªÉm tra s·ªë l∆∞·ª£ng t√†i li·ªáu trong ChromaDB\n",
    "print(f\"üìù S·ªë l∆∞·ª£ng t√†i li·ªáu trong ChromaDB: {collection.count()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_documents_periodically():\n",
    "    \"\"\"Ki·ªÉm tra v√† t·∫£i t√†i li·ªáu m·ªõi ƒë·ªãnh k·ª≥\"\"\"\n",
    "    while True:\n",
    "        print(\"ƒêang ki·ªÉm tra t√†i li·ªáu m·ªõi t·ª´ SharePoint...\")\n",
    "        pdf_files = get_files_in_folder() \n",
    "        for file in pdf_files:\n",
    "            if file not in downloaded_files:\n",
    "                download_file(file) \n",
    "                downloaded_files.append(file)\n",
    "        time.sleep(7200)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_chromadb_with_new_documents(pdf_files):\n",
    "    \"\"\"C·∫≠p nh·∫≠t t√†i li·ªáu m·ªõi v√†o ChromaDB\"\"\"\n",
    "    for file_name in pdf_files:\n",
    "        pdf_path = f\"downloads/{file_name}\"\n",
    "        text = extract_text_from_pdf(pdf_path) \n",
    "\n",
    "        if text.strip():  \n",
    "            collection.add(\n",
    "                documents=[text],\n",
    "                embeddings=[get_embedding(text)],\n",
    "                ids=[file_name]\n",
    "            )\n",
    "            print(f\"‚úÖ ƒê√£ c·∫≠p nh·∫≠t v√†o ChromaDB: {file_name}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è File {file_name} kh√¥ng c√≥ n·ªôi dung!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_user_feedback(query, answer):\n",
    "    \"\"\"Thu th·∫≠p ph·∫£n h·ªìi t·ª´ ng∆∞·ªùi d√πng v√† l∆∞u l·∫°i v√†o file\"\"\"\n",
    "    feedback = input(\"C√¢u tr·∫£ l·ªùi n√†y c√≥ ch√≠nh x√°c kh√¥ng? (yes/no): \").lower()\n",
    "    \n",
    "    if feedback in [\"yes\", \"y\"]:\n",
    "        print(\"C·∫£m ∆°n b·∫°n!\")\n",
    "        # L∆∞u l·∫°i ph·∫£n h·ªìi ch√≠nh x√°c n·∫øu c·∫ßn (n·∫øu b·∫°n mu·ªën l∆∞u)\n",
    "        return True\n",
    "    else:\n",
    "        print(\"C·∫£m ∆°n b·∫°n ƒë√£ ph·∫£n h·ªìi! T√¥i s·∫Ω c·∫£i thi·ªán.\")\n",
    "        # L∆∞u l·∫°i c√°c c√¢u tr·∫£ l·ªùi kh√¥ng ch√≠nh x√°c ƒë·ªÉ ph√¢n t√≠ch\n",
    "        with open(\"feedback_log.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Question: {query}, Answer: {answer}, Feedback: Incorrect\\n\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_chroma(query, top_k=3):\n",
    "    embedding = get_embedding(query)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    # Tr·∫£ v·ªÅ danh s√°ch c√°c vƒÉn b·∫£n, kh√¥ng ph·∫£i danh s√°ch con\n",
    "    return [result[0] for result in results['documents']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n vƒÉn b·∫£n t·ª´ ChromaDB\n",
    "def generate_answer(query):\n",
    "    context = \"\\n\".join(search_in_chroma(query))  # K·∫øt h·ª£p c√°c ƒëo·∫°n vƒÉn b·∫£n th√†nh m·ªôt chu·ªói\n",
    "    if context:\n",
    "        prompt = f\"Tr·∫£ l·ªùi c√¢u h·ªèi sau d·ª±a tr√™n th√¥ng tin d∆∞·ªõi ƒë√¢y:\\n\\n{context}\\n\\nC√¢u h·ªèi: {query}\\nTr·∫£ l·ªùi:\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",  # Ch·ªçn m√¥ h√¨nh gpt-3.5-turbo thay v√¨ text-davinci-003\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"B·∫°n l√† m·ªôt tr·ª£ l√Ω th√¥ng minh.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=150\n",
    "        )\n",
    "        return response['choices'][0]['message']['content'].strip()\n",
    "    else:\n",
    "        return \"Xin l·ªói, t√¥i kh√¥ng th·ªÉ t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·∫øn c√¢u h·ªèi c·ªßa b·∫°n.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    print(\"Ch√†o b·∫°n! T√¥i l√† tr·ª£ l√Ω ·∫£o c·ªßa Mai Th∆∞. B·∫°n c√≥ th·ªÉ h·ªèi t√¥i b·∫•t c·ª© c√¢u h·ªèi n√†o.\")\n",
    "    while True:\n",
    "        user_input = input(\"B·∫°n: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"T·∫°m bi·ªát!\")\n",
    "            break\n",
    "\n",
    "        print(f\"\\nC√¢u h·ªèi c·ªßa b·∫°n: {user_input}\\n\")\n",
    "        answer = generate_answer(user_input) \n",
    "        \n",
    "        print(f\"Chatbot: {answer}\")\n",
    "        \n",
    "        collect_user_feedback(user_input, answer)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ch√†o b·∫°n! T√¥i l√† tr·ª£ l√Ω ·∫£o c·ªßa Mai Th∆∞. B·∫°n c√≥ th·ªÉ h·ªèi t√¥i b·∫•t c·ª© c√¢u h·ªèi n√†o.\n",
      "\n",
      "C√¢u h·ªèi c·ªßa b·∫°n: i need some informations of Take away box\n",
      "\n",
      "Chatbot: Th√¥ng tin v·ªÅ h·ªôp ƒë·ª±ng th·ª©c ƒÉn mang v·ªÅ (Take away box) bao g·ªìm:\n",
      "- V·∫≠t li·ªáu: Kraft, Ivory, gi·∫•y carton 3 l·ªõp,...\n",
      "- Tr·ªçng l∆∞·ª£ng gi·∫•y: 250gsm - 400gsm,...\n",
      "- Ph∆∞∆°ng ph√°p in: Offset / Flexo, 1 m√†u / nhi·ªÅu m√†u\n",
      "- Ki·ªÉu d√°ng: C√≥ c·ª≠a s·ªï / kh√¥ng c√≥ c·ª≠a s·ªï\n",
      "- C√°c k√≠ch th∆∞·ªõc v√† lo·∫°i h·ªôp kh√°c nhau nh∆∞ h·ªôp c·ªëc coffee mang v·ªÅ, h·ªô\n",
      "C·∫£m ∆°n b·∫°n!\n",
      "\n",
      "C√¢u h·ªèi c·ªßa b·∫°n: what about  Cup carrier tray\n",
      "\n",
      "Chatbot: Cup carrier tray is one of the products in Mai Thu's product range. It is made from white kraft and brown kraft with a paper weight of 350gsm. The printing method used for Cup carrier tray is Offset/Flexo, and it comes in a style with 2 cups or 4 cups.\n",
      "C·∫£m ∆°n b·∫°n ƒë√£ ph·∫£n h·ªìi! T√¥i s·∫Ω c·∫£i thi·ªán.\n",
      "\n",
      "C√¢u h·ªèi c·ªßa b·∫°n: and bread bag\n",
      "\n",
      "Chatbot: Bread bag.\n",
      "C·∫£m ∆°n b·∫°n ƒë√£ ph·∫£n h·ªìi! T√¥i s·∫Ω c·∫£i thi·ªán.\n",
      "\n",
      "C√¢u h·ªèi c·ªßa b·∫°n: Do mai thu packaging produce toys for kids?\n",
      "\n",
      "Chatbot: D·ª±a v√†o th√¥ng tin ƒë√£ cung c·∫•p, Mai Thu Packaging s·∫£n xu·∫•t nh·ªØng s·∫£n ph·∫©m nh∆∞ h·ªôp ƒë·ª±ng th·ª©c ƒÉn mang ƒëi, h·ªôp b√°nh pizza, t√∫i b√°nh m·ª≥, ƒë·ªì d√πng gi·∫•y v√† ph·ª• ki·ªán, ƒë·ªì ch∆°i nh√† gi·∫•y cho tr·∫ª em, nh√† cho th√∫ c∆∞ng v√† b·ªô ƒë·ªì ch∆°i d√†nh cho tr·∫ª em. V√¨ v·∫≠y, c√≥ th·ªÉ n√≥i r·∫±ng Mai Thu Packaging s·∫£n xu·∫•t ƒë·ªì ch∆°i cho tr·∫ª em.\n",
      "C·∫£m ∆°n b·∫°n ƒë√£ ph·∫£n h·ªìi! T√¥i s·∫Ω c·∫£i thi·ªán.\n",
      "\n",
      "C√¢u h·ªèi c·ªßa b·∫°n: nh·ªØng ƒë·ªì ch∆°i n√†o ƒë∆∞·ª£c s·∫£n xu·∫•t b·ªüi mai th∆∞\n",
      "\n",
      "Chatbot: C√¥ng ty C·ªï ph·∫ßn Bao B√¨ Mai Th∆∞ s·∫£n xu·∫•t c√°c s·∫£n ph·∫©m bao b√¨ gi·∫•y in offset, bao b√¨ gi·∫•y in offset b·ªìi carton, bao b√¨ gi·∫•y carton, bao b√¨ gi·∫•y cho th·ª±c ph·∫©m, s·∫£n ph·∫©m gi·∫•y g√≥i qu√†, c≈©ng nh∆∞ c√°c s·∫£n ph·∫©m c√¥ng nghi·ªáp gi·∫•y kh√°c.\n",
      "C·∫£m ∆°n b·∫°n ƒë√£ ph·∫£n h·ªìi! T√¥i s·∫Ω c·∫£i thi·ªán.\n",
      "\n",
      "C√¢u h·ªèi c·ªßa b·∫°n: li·ªát k√™  CH·∫æ ƒê·ªò PH√öC L·ª¢I\n",
      "\n",
      "Chatbot: C√¥ng ty C·ªï ph·∫ßn Bao B√¨ Mai Th∆∞ cung c·∫•p c√°c ch·∫ø ƒë·ªô ph√∫c l·ª£i nh∆∞ sau:\n",
      "\n",
      "1. Th∆∞·ªüng c√°c d·ªãp L·ªÖ l·ªõn nh∆∞ Ng√†y Qu·ªëc t·∫ø Ph·ª• n·ªØ, L·ªÖ H√πng V∆∞∆°ng, Ng√†y Qu·ªëc Kh√°nh, T·∫øt D∆∞∆°ng L·ªãch.\n",
      "2. H·ªó tr·ª£ h·ªçc tr√≤ con c·ªßa c√°n b·ªô nh√¢n vi√™n.\n",
      "3. H·ªó tr·ª£ thƒÉm h·ªèi ƒë·ªëi v·ªõi c√°c s·ª± ki·ªán nh∆∞ sinh nh·∫≠t, sinh em b√©, ·ªëm ƒëau, k·∫øt h√¥n,\n",
      "C·∫£m ∆°n b·∫°n!\n",
      "\n",
      "C√¢u h·ªèi c·ªßa b·∫°n: Li·ªát k√™ ƒë·∫ßy ƒë·ªß t·∫•t c·∫£ ph√∫c l·ª£i c·ªßa c√¥ng ty\n",
      "\n",
      "Chatbot: C√¥ng ty C·ªï ph·∫ßn Bao B√¨ Mai Th∆∞ cung c·∫•p c√°c ch√≠nh s√°ch v√† ch·∫ø ƒë·ªô ph√∫c l·ª£i sau cho nh√¢n vi√™n:\n",
      "\n",
      "1. Th∆∞·ªüng c√°c d·ªãp L·ªÖ l·ªõn nh∆∞ Ng√†y Qu·ªëc t·∫ø Ph·ª• n·ªØ 8/3, L·ªÖ H√πng V∆∞∆°ng v√† 30/04 - 01/05, L·ªÖ Qu·ªëc Kh√°nh 2/9, T·∫øt D∆∞∆°ng L·ªãch v·ªõi m·ª©c th∆∞·ªüng kh√°c nhau t√πy theo th√¢m ni√™n l√†m vi·ªác t·∫°i c√¥ng ty.\n",
      "2. H·ªó tr\n",
      "C·∫£m ∆°n b·∫°n ƒë√£ ph·∫£n h·ªìi! T√¥i s·∫Ω c·∫£i thi·ªán.\n",
      "\n",
      "C√¢u h·ªèi c·ªßa b·∫°n: g√≥i b·∫£o hi·ªÉm s·ª©c kh·ªèe c·ªßa c√¥ng ty nh∆∞ th·∫ø n√†o\n",
      "\n",
      "Chatbot: C√¥ng ty C·ªï ph·∫ßn Bao B√¨ Mai Th∆∞ c√≥ c√°c g√≥i b·∫£o hi·ªÉm s·ª©c kh·ªèe nh∆∞ sau:\n",
      "- G√≥i s·ªë 1: D√†nh cho c√°n b·ªô, c√¥ng nh√¢n vi√™n c√≥ th√¢m ni√™n t·ª´ 1 ƒë·∫øn 5 nƒÉm v·ªõi m·ª©c ph√≠ 2.556.000 VNƒê/ng∆∞·ªùi.\n",
      "- G√≥i s·ªë 2: D√†nh cho c√°n b·ªô, c√¥ng nh√¢n vi√™n c√≥ th√¢m ni√™n t·ª´ 5 ƒë·∫øn 10 nƒÉm v·ªõi m·ª©c ph√≠ 4.725.000 VNƒê/ng∆∞·ªùi.\n",
      "- G√≥i s·ªë 3: D√†nh cho c√°n b·ªô\n",
      "C·∫£m ∆°n b·∫°n ƒë√£ ph·∫£n h·ªìi! T√¥i s·∫Ω c·∫£i thi·ªán.\n",
      "T·∫°m bi·ªát!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
